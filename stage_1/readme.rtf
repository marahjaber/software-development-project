20191117
first draft

20191118
1) run program with original logs, without copy all logs from original directory to a new folder
2) output the predction result into files

20191119
1) change search_error function into insert_label
2) do shuffle to randomize the input logs instead of using train_test_split with random feature before labeling and vectorizing the text
   therefore, we can know the test string is located in the back of the all input logs (for example the first 80% is train string and last 20% is test string)
   to prevent large program execution time for finding the each test string from all input logs

20191120
1) read all logs input list instead of csv
2) remove copy_data and read_data function

20191121
1) add more types of abnormal events
2) add gen_artificial_log function for text augmentation
3) put output file into a new directory

20191122
1) add more types of abnormal events
2) output file with original file name and line number

20191128
1) change validation_split from 0.1 to 0.2
2) add more artificial log
3) filter artificial log in test string during output phase

20191128
1)stage 1-2 first draft

20191201
1)add stopwords for uesless words and digits
2)extract 11 lines from raw logs for each abnormal event
3)insert user priority list if the user defined keywords exist

20191202
1)add stopword file for improving code management

20191203
1)integrate line_classify_20191128 and tfidf_20191202

20191215
1)use template log for training data set

20191217
1)add new 136 logs

20191222
1)separate train and test data folder
